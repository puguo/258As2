{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Top-K Restaurant Recommendation\n",
    "\n",
    "This notebook preprocesses the Google restaurant review dataset for building a similarity-based top-K recommendation system.\n",
    "\n",
    "**Preprocessing Steps:**\n",
    "1. Load and clean the raw data\n",
    "2. Filter sparse users and businesses\n",
    "3. Create ID mappings for efficient indexing\n",
    "4. Split into train/validation/test sets\n",
    "5. Create sparse interaction matrices\n",
    "6. Compute business statistics for cold-start handling\n",
    "7. Save preprocessed data for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA PREPROCESSING FOR TOP-K RESTAURANT RECOMMENDATION\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Load the raw Google restaurant review data from JSON Lines format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[1/7] Loading dataset...\")\n",
    "data = []\n",
    "with open('google_restaraunt.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"   Total reviews: {len(df):,}\")\n",
    "print(f\"   Unique users: {df['user_id'].nunique():,}\")\n",
    "print(f\"   Unique businesses: {df['business_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean Data\n",
    "\n",
    "Remove duplicates and handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[2/7] Cleaning data...\")\n",
    "# Remove any duplicates\n",
    "df = df.drop_duplicates(subset=['user_id', 'business_id'])\n",
    "print(f\"   After removing duplicates: {len(df):,} reviews\")\n",
    "\n",
    "# Check for missing values\n",
    "missing = df[['user_id', 'business_id', 'rating']].isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(f\"   Found missing values: {missing[missing > 0]}\")\n",
    "    df = df.dropna(subset=['user_id', 'business_id', 'rating'])\n",
    "    print(f\"   After removing missing: {len(df):,} reviews\")\n",
    "else:\n",
    "    print(\"   No missing values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter Sparse Users and Businesses\n",
    "\n",
    "To improve recommendation quality, we filter out:\n",
    "- Users with fewer than 3 reviews\n",
    "- Businesses with fewer than 5 reviews\n",
    "\n",
    "This is done iteratively until convergence, as filtering one affects the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[3/7] Filtering sparse users and businesses...\")\n",
    "min_user_reviews = 3  # Users must have at least 3 reviews\n",
    "min_business_reviews = 5  # Businesses must have at least 5 reviews\n",
    "\n",
    "# Iteratively filter (because filtering businesses affects users and vice versa)\n",
    "prev_size = 0\n",
    "iteration = 0\n",
    "while len(df) != prev_size and iteration < 10:\n",
    "    prev_size = len(df)\n",
    "    iteration += 1\n",
    "    \n",
    "    user_counts = df['user_id'].value_counts()\n",
    "    business_counts = df['business_id'].value_counts()\n",
    "    \n",
    "    valid_users = user_counts[user_counts >= min_user_reviews].index\n",
    "    valid_businesses = business_counts[business_counts >= min_business_reviews].index\n",
    "    \n",
    "    df = df[df['user_id'].isin(valid_users) & df['business_id'].isin(valid_businesses)]\n",
    "    \n",
    "    if iteration > 1:\n",
    "        print(f\"   Iteration {iteration}: {len(df):,} reviews\")\n",
    "\n",
    "print(f\"\\n   Final dataset after filtering:\")\n",
    "print(f\"   Reviews: {len(df):,}\")\n",
    "print(f\"   Users: {df['user_id'].nunique():,}\")\n",
    "print(f\"   Businesses: {df['business_id'].nunique():,}\")\n",
    "print(f\"   Sparsity: {(1 - len(df)/(df['user_id'].nunique() * df['business_id'].nunique()))*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create ID Mappings\n",
    "\n",
    "Map user and business IDs to integer indices for efficient matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[4/7] Creating ID mappings...\")\n",
    "unique_users = df['user_id'].unique()\n",
    "unique_businesses = df['business_id'].unique()\n",
    "\n",
    "user_to_idx = {uid: idx for idx, uid in enumerate(unique_users)}\n",
    "idx_to_user = {idx: uid for uid, idx in user_to_idx.items()}\n",
    "\n",
    "business_to_idx = {bid: idx for idx, bid in enumerate(unique_businesses)}\n",
    "idx_to_business = {idx: bid for bid, idx in business_to_idx.items()}\n",
    "\n",
    "# Add mapped indices to dataframe\n",
    "df['user_idx'] = df['user_id'].map(user_to_idx)\n",
    "df['business_idx'] = df['business_id'].map(business_to_idx)\n",
    "\n",
    "print(f\"   Created mappings for {len(user_to_idx):,} users and {len(business_to_idx):,} businesses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split into Train/Validation/Test Sets\n",
    "\n",
    "We use per-user splitting to ensure:\n",
    "- Each user appears in the training set\n",
    "- We can evaluate on held-out user interactions\n",
    "\n",
    "**Split Strategy:**\n",
    "- Users with ≥5 reviews: 70% train, 15% val, 15% test\n",
    "- Users with <5 reviews: All but 1 in train, 1 in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[5/7] Splitting into train/validation/test sets...\")\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "for user_id in df['user_id'].unique():\n",
    "    user_reviews = df[df['user_id'] == user_id]\n",
    "    n_reviews = len(user_reviews)\n",
    "    \n",
    "    if n_reviews >= 5:\n",
    "        # For users with many reviews: 70% train, 15% val, 15% test\n",
    "        train_size = int(0.7 * n_reviews)\n",
    "        val_size = int(0.15 * n_reviews)\n",
    "        \n",
    "        # Shuffle user's reviews\n",
    "        user_reviews = user_reviews.sample(frac=1, random_state=42)\n",
    "        \n",
    "        train_data.append(user_reviews.iloc[:train_size])\n",
    "        val_data.append(user_reviews.iloc[train_size:train_size+val_size])\n",
    "        test_data.append(user_reviews.iloc[train_size+val_size:])\n",
    "    else:\n",
    "        # For users with few reviews: put most in train, 1 in test\n",
    "        user_reviews = user_reviews.sample(frac=1, random_state=42)\n",
    "        train_data.append(user_reviews.iloc[:-1])\n",
    "        test_data.append(user_reviews.iloc[-1:])\n",
    "\n",
    "train_df = pd.concat(train_data, ignore_index=True)\n",
    "val_df = pd.concat(val_data, ignore_index=True) if val_data else pd.DataFrame()\n",
    "test_df = pd.concat(test_data, ignore_index=True)\n",
    "\n",
    "print(f\"   Train set: {len(train_df):,} reviews ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Validation set: {len(val_df):,} reviews ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Test set: {len(test_df):,} reviews ({len(test_df)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Sparse Interaction Matrices\n",
    "\n",
    "Create user-item interaction matrices in sparse format for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[6/7] Creating interaction matrices...\")\n",
    "\n",
    "def create_interaction_matrix(df_subset, n_users, n_businesses):\n",
    "    \"\"\"Create sparse user-item interaction matrix\"\"\"\n",
    "    rows = df_subset['user_idx'].values\n",
    "    cols = df_subset['business_idx'].values\n",
    "    ratings = df_subset['rating'].values\n",
    "    \n",
    "    matrix = csr_matrix((ratings, (rows, cols)), shape=(n_users, n_businesses))\n",
    "    return matrix\n",
    "\n",
    "n_users = len(user_to_idx)\n",
    "n_businesses = len(business_to_idx)\n",
    "\n",
    "train_matrix = create_interaction_matrix(train_df, n_users, n_businesses)\n",
    "val_matrix = create_interaction_matrix(val_df, n_users, n_businesses) if len(val_df) > 0 else None\n",
    "test_matrix = create_interaction_matrix(test_df, n_users, n_businesses)\n",
    "\n",
    "print(f\"   Train matrix shape: {train_matrix.shape}\")\n",
    "print(f\"   Train matrix density: {train_matrix.nnz / (train_matrix.shape[0] * train_matrix.shape[1]) * 100:.4f}%\")\n",
    "if val_matrix is not None:\n",
    "    print(f\"   Val matrix shape: {val_matrix.shape}\")\n",
    "print(f\"   Test matrix shape: {test_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compute Business Statistics\n",
    "\n",
    "Calculate statistics for each business to handle cold-start scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[7/7] Computing business statistics...\")\n",
    "business_stats = train_df.groupby('business_id').agg({\n",
    "    'rating': ['mean', 'count', 'std']\n",
    "}).reset_index()\n",
    "business_stats.columns = ['business_id', 'avg_rating', 'num_ratings', 'std_rating']\n",
    "business_stats['std_rating'] = business_stats['std_rating'].fillna(0)\n",
    "\n",
    "# Add business index\n",
    "business_stats['business_idx'] = business_stats['business_id'].map(business_to_idx)\n",
    "\n",
    "print(f\"   Computed statistics for {len(business_stats):,} businesses\")\n",
    "print(f\"   Average rating across all businesses: {business_stats['avg_rating'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Preprocessed Data\n",
    "\n",
    "Save all preprocessed data structures for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVING PREPROCESSED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save as pickle for fast loading\n",
    "save_data = {\n",
    "    'train_df': train_df,\n",
    "    'val_df': val_df,\n",
    "    'test_df': test_df,\n",
    "    'train_matrix': train_matrix,\n",
    "    'val_matrix': val_matrix,\n",
    "    'test_matrix': test_matrix,\n",
    "    'user_to_idx': user_to_idx,\n",
    "    'idx_to_user': idx_to_user,\n",
    "    'business_to_idx': business_to_idx,\n",
    "    'idx_to_business': idx_to_business,\n",
    "    'business_stats': business_stats,\n",
    "    'n_users': n_users,\n",
    "    'n_businesses': n_businesses,\n",
    "}\n",
    "\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(save_data, f)\n",
    "print(\"✓ Saved preprocessed_data.pkl\")\n",
    "\n",
    "# Also save CSV versions for easy inspection\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "if len(val_df) > 0:\n",
    "    val_df.to_csv('val_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "business_stats.to_csv('business_stats.csv', index=False)\n",
    "print(\"✓ Saved CSV files (train_data.csv, val_data.csv, test_data.csv, business_stats.csv)\")\n",
    "\n",
    "# Save mappings as JSON for readability\n",
    "mappings = {\n",
    "    'user_to_idx': user_to_idx,\n",
    "    'business_to_idx': business_to_idx,\n",
    "}\n",
    "with open('id_mappings.json', 'w') as f:\n",
    "    json.dump(mappings, f, indent=2)\n",
    "print(\"✓ Saved id_mappings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Preprocessing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total users: {n_users:,}\")\n",
    "print(f\"Total businesses: {n_businesses:,}\")\n",
    "print(f\"Total interactions: {len(df):,}\")\n",
    "print(f\"\")\n",
    "print(f\"Train interactions: {len(train_df):,}\")\n",
    "print(f\"Val interactions: {len(val_df):,}\")\n",
    "print(f\"Test interactions: {len(test_df):,}\")\n",
    "print(f\"\")\n",
    "print(f\"Avg reviews per user: {len(train_df)/n_users:.2f}\")\n",
    "print(f\"Avg reviews per business: {len(train_df)/n_businesses:.2f}\")\n",
    "print(f\"\")\n",
    "print(f\"Rating distribution (train):\")\n",
    "for rating in sorted(train_df['rating'].unique()):\n",
    "    count = (train_df['rating'] == rating).sum()\n",
    "    pct = count / len(train_df) * 100\n",
    "    print(f\"  {rating} stars: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n✓ Preprocessing complete!\")\n",
    "print(\"\\nTo load the preprocessed data:\")\n",
    "print(\"  import pickle\")\n",
    "print(\"  with open('preprocessed_data.pkl', 'rb') as f:\")\n",
    "print(\"      data = pickle.load(f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "With the preprocessed data, you can now:\n",
    "\n",
    "1. **Build similarity-based recommendation model**\n",
    "   - Compute item-item similarity matrix\n",
    "   - Implement top-K recommendation function\n",
    "\n",
    "2. **Evaluate model performance**\n",
    "   - Precision@K\n",
    "   - Recall@K\n",
    "   - NDCG@K\n",
    "   - MAP@K\n",
    "\n",
    "3. **Handle cold-start problems**\n",
    "   - Use business statistics for new users\n",
    "   - Use popularity-based recommendations as baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
