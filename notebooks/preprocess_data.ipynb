{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Top-K Restaurant Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sp\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-02 10:15:59--  https://mcauleylab.ucsd.edu/public_datasets/gdrive/googlelocal_restaurants/image_review_all.json\n",
      "Resolving mcauleylab.ucsd.edu (mcauleylab.ucsd.edu)... 169.228.63.88\n",
      "Connecting to mcauleylab.ucsd.edu (mcauleylab.ucsd.edu)|169.228.63.88|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1087842416 (1.0G) [application/json]\n",
      "Saving to: ‘image_review_all.json.1’\n",
      "\n",
      "image_review_all.js 100%[===================>]   1.01G  93.9MB/s    in 11s     \n",
      "\n",
      "2025-12-02 10:16:10 (94.4 MB/s) - ‘image_review_all.json.1’ saved [1087842416/1087842416]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://mcauleylab.ucsd.edu/public_datasets/gdrive/googlelocal_restaurants/image_review_all.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Load the raw Google restaurant review data from JSON Lines format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/7] Loading dataset...\n",
      "   Total reviews: 1,487,747\n",
      "   Unique users: 868,937\n",
      "   Unique businesses: 64,527\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[1/7] Loading dataset...\")\n",
    "data = []\n",
    "with open('image_review_all.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"   Total reviews: {len(df):,}\")\n",
    "print(f\"   Unique users: {df['user_id'].nunique():,}\")\n",
    "print(f\"   Unique businesses: {df['business_id'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean Data\n",
    "\n",
    "Remove duplicates and handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/7] Cleaning data...\n",
      "   After removing duplicates: 1,487,747 reviews\n",
      "   No missing values found\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2/7] Cleaning data...\")\n",
    "# Remove any duplicates\n",
    "df = df.drop_duplicates(subset=['user_id', 'business_id'])\n",
    "print(f\"   After removing duplicates: {len(df):,} reviews\")\n",
    "\n",
    "# Check for missing values\n",
    "missing = df[['user_id', 'business_id', 'rating']].isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(f\"   Found missing values: {missing[missing > 0]}\")\n",
    "    df = df.dropna(subset=['user_id', 'business_id', 'rating'])\n",
    "    print(f\"   After removing missing: {len(df):,} reviews\")\n",
    "else:\n",
    "    print(\"   No missing values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter Sparse Users and Businesses\n",
    "\n",
    "To improve recommendation quality, we filter out:\n",
    "- Users with fewer than 3 reviews\n",
    "- Businesses with fewer than 5 reviews\n",
    "\n",
    "This is done iteratively until convergence, as filtering one affects the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/7] Filtering sparse users and businesses...\n",
      "   Iteration 2: 544,416 reviews\n",
      "   Iteration 3: 523,230 reviews\n",
      "   Iteration 4: 517,958 reviews\n",
      "   Iteration 5: 515,268 reviews\n",
      "   Iteration 6: 514,479 reviews\n",
      "   Iteration 7: 514,046 reviews\n",
      "   Iteration 8: 513,936 reviews\n",
      "   Iteration 9: 513,872 reviews\n",
      "   Iteration 10: 513,868 reviews\n",
      "\n",
      "   Final dataset after filtering:\n",
      "   Reviews: 513,868\n",
      "   Users: 98,975\n",
      "   Businesses: 28,274\n",
      "   Sparsity: 99.9816%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[3/7] Filtering sparse users and businesses...\")\n",
    "min_user_reviews = 3  # Users must have at least 3 reviews\n",
    "min_business_reviews = 5  # Businesses must have at least 5 reviews\n",
    "\n",
    "# Iteratively filter (because filtering businesses affects users and vice versa)\n",
    "prev_size = 0\n",
    "iteration = 0\n",
    "while len(df) != prev_size and iteration < 10:\n",
    "    prev_size = len(df)\n",
    "    iteration += 1\n",
    "    \n",
    "    user_counts = df['user_id'].value_counts()\n",
    "    business_counts = df['business_id'].value_counts()\n",
    "    \n",
    "    valid_users = user_counts[user_counts >= min_user_reviews].index\n",
    "    valid_businesses = business_counts[business_counts >= min_business_reviews].index\n",
    "    \n",
    "    df = df[df['user_id'].isin(valid_users) & df['business_id'].isin(valid_businesses)]\n",
    "    \n",
    "    if iteration > 1:\n",
    "        print(f\"   Iteration {iteration}: {len(df):,} reviews\")\n",
    "\n",
    "print(f\"\\n   Final dataset after filtering:\")\n",
    "print(f\"   Reviews: {len(df):,}\")\n",
    "print(f\"   Users: {df['user_id'].nunique():,}\")\n",
    "print(f\"   Businesses: {df['business_id'].nunique():,}\")\n",
    "print(f\"   Sparsity: {(1 - len(df)/(df['user_id'].nunique() * df['business_id'].nunique()))*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create ID Mappings\n",
    "\n",
    "Map user and business IDs to integer indices for efficient matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/7] Creating ID mappings...\n",
      "   Created mappings for 98,975 users and 28,274 businesses\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[4/7] Creating ID mappings...\")\n",
    "unique_users = df['user_id'].unique()\n",
    "unique_businesses = df['business_id'].unique()\n",
    "\n",
    "user_to_idx = {uid: idx for idx, uid in enumerate(unique_users)}\n",
    "idx_to_user = {idx: uid for uid, idx in user_to_idx.items()}\n",
    "\n",
    "business_to_idx = {bid: idx for idx, bid in enumerate(unique_businesses)}\n",
    "idx_to_business = {idx: bid for bid, idx in business_to_idx.items()}\n",
    "\n",
    "# Add mapped indices to dataframe\n",
    "df['user_idx'] = df['user_id'].map(user_to_idx)\n",
    "df['business_idx'] = df['business_id'].map(business_to_idx)\n",
    "\n",
    "print(f\"   Created mappings for {len(user_to_idx):,} users and {len(business_to_idx):,} businesses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Split into Train/Validation/Test Sets\n",
    "\n",
    "We use per-user splitting to ensure:\n",
    "- Each user appears in the training set\n",
    "- We can evaluate on held-out user interactions\n",
    "\n",
    "**Split Strategy:**\n",
    "- Users with ≥5 reviews: 70% train, 15% val, 15% test\n",
    "- Users with <5 reviews: All but 1 in train, 1 in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/7] Splitting into train/validation/test sets...\n",
      "Train: 342,665\n",
      "Val: 24,950\n",
      "Test: 146,253\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[5/7] Splitting into train/validation/test sets...\")\n",
    "\n",
    "train_rows = []\n",
    "val_rows = []\n",
    "test_rows = []\n",
    "\n",
    "grouped = df.groupby(\"user_id\", sort=False)\n",
    "\n",
    "for user_id, group in grouped:\n",
    "    group = group.sample(frac=1, random_state=42)  # shuffle once\n",
    "    n = len(group)\n",
    "\n",
    "    if n >= 5:\n",
    "        train_size = int(0.7 * n)\n",
    "        val_size = int(0.15 * n)\n",
    "\n",
    "        train_rows.append(group.iloc[:train_size])\n",
    "        val_rows.append(group.iloc[train_size:train_size + val_size])\n",
    "        test_rows.append(group.iloc[train_size + val_size:])\n",
    "    else:\n",
    "        train_rows.append(group.iloc[:-1])\n",
    "        test_rows.append(group.iloc[-1:])\n",
    "\n",
    "train_df = pd.concat(train_rows, ignore_index=True)\n",
    "val_df = pd.concat(val_rows, ignore_index=True)\n",
    "test_df = pd.concat(test_rows, ignore_index=True)\n",
    "\n",
    "print(f\"Train: {len(train_df):,}\")\n",
    "print(f\"Val: {len(val_df):,}\")\n",
    "print(f\"Test: {len(test_df):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Sparse Interaction Matrices\n",
    "\n",
    "Create user-item interaction matrices in sparse format for memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/7] Creating interaction matrices...\n",
      "   Train matrix shape: (98975, 28274)\n",
      "   Train matrix density: 0.0122%\n",
      "   Val matrix shape: (98975, 28274)\n",
      "   Test matrix shape: (98975, 28274)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[6/7] Creating interaction matrices...\")\n",
    "\n",
    "def create_interaction_matrix(df_subset, n_users, n_businesses):\n",
    "    \"\"\"Create sparse user-item interaction matrix\"\"\"\n",
    "    rows = df_subset['user_idx'].values\n",
    "    cols = df_subset['business_idx'].values\n",
    "    ratings = df_subset['rating'].values\n",
    "    \n",
    "    matrix = csr_matrix((ratings, (rows, cols)), shape=(n_users, n_businesses))\n",
    "    return matrix\n",
    "\n",
    "n_users = len(user_to_idx)\n",
    "n_businesses = len(business_to_idx)\n",
    "\n",
    "train_matrix = create_interaction_matrix(train_df, n_users, n_businesses)\n",
    "val_matrix = create_interaction_matrix(val_df, n_users, n_businesses) if len(val_df) > 0 else None\n",
    "test_matrix = create_interaction_matrix(test_df, n_users, n_businesses)\n",
    "\n",
    "print(f\"   Train matrix shape: {train_matrix.shape}\")\n",
    "print(f\"   Train matrix density: {train_matrix.nnz / (train_matrix.shape[0] * train_matrix.shape[1]) * 100:.4f}%\")\n",
    "if val_matrix is not None:\n",
    "    print(f\"   Val matrix shape: {val_matrix.shape}\")\n",
    "print(f\"   Test matrix shape: {test_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compute Business Statistics\n",
    "\n",
    "Calculate statistics for each business to handle cold-start scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7/7] Computing business statistics...\n",
      "   Computed statistics for 28,223 businesses\n",
      "   Average rating across all businesses: 4.434\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[7/7] Computing business statistics...\")\n",
    "business_stats = train_df.groupby('business_id').agg({\n",
    "    'rating': ['mean', 'count', 'std']\n",
    "}).reset_index()\n",
    "business_stats.columns = ['business_id', 'avg_rating', 'num_ratings', 'std_rating']\n",
    "business_stats['std_rating'] = business_stats['std_rating'].fillna(0)\n",
    "\n",
    "# Add business index\n",
    "business_stats['business_idx'] = business_stats['business_id'].map(business_to_idx)\n",
    "\n",
    "print(f\"   Computed statistics for {len(business_stats):,} businesses\")\n",
    "print(f\"   Average rating across all businesses: {business_stats['avg_rating'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Preprocessed Data\n",
    "\n",
    "Save all preprocessed data structures for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING PREPROCESSED DATA\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved preprocessed_data.pkl\n",
      "✓ Saved CSV files (train_data.csv, val_data.csv, test_data.csv, business_stats.csv)\n",
      "✓ Saved id_mappings.json\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVING PREPROCESSED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save as pickle for fast loading\n",
    "save_data = {\n",
    "    'train_df': train_df,\n",
    "    'val_df': val_df,\n",
    "    'test_df': test_df,\n",
    "    'train_matrix': train_matrix,\n",
    "    'val_matrix': val_matrix,\n",
    "    'test_matrix': test_matrix,\n",
    "    'user_to_idx': user_to_idx,\n",
    "    'idx_to_user': idx_to_user,\n",
    "    'business_to_idx': business_to_idx,\n",
    "    'idx_to_business': idx_to_business,\n",
    "    'business_stats': business_stats,\n",
    "    'n_users': n_users,\n",
    "    'n_businesses': n_businesses,\n",
    "}\n",
    "\n",
    "with open('preprocessed_data.pkl', 'wb') as f:\n",
    "    pickle.dump(save_data, f)\n",
    "print(\"✓ Saved preprocessed_data.pkl\")\n",
    "\n",
    "# Also save CSV versions for easy inspection\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "if len(val_df) > 0:\n",
    "    val_df.to_csv('val_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "business_stats.to_csv('business_stats.csv', index=False)\n",
    "print(\"✓ Saved CSV files (train_data.csv, val_data.csv, test_data.csv, business_stats.csv)\")\n",
    "\n",
    "# Save mappings as JSON for readability\n",
    "mappings = {\n",
    "    'user_to_idx': user_to_idx,\n",
    "    'business_to_idx': business_to_idx,\n",
    "}\n",
    "with open('id_mappings.json', 'w') as f:\n",
    "    json.dump(mappings, f, indent=2)\n",
    "print(\"✓ Saved id_mappings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Preprocessing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PREPROCESSING SUMMARY\n",
      "============================================================\n",
      "Total users: 98,975\n",
      "Total businesses: 28,274\n",
      "Total interactions: 513,868\n",
      "\n",
      "Train interactions: 342,665\n",
      "Val interactions: 24,950\n",
      "Test interactions: 146,253\n",
      "\n",
      "Avg reviews per user: 3.46\n",
      "Avg reviews per business: 12.12\n",
      "\n",
      "Rating distribution (train):\n",
      "  1 stars: 4,671 (1.4%)\n",
      "  2 stars: 7,455 (2.2%)\n",
      "  3 stars: 25,654 (7.5%)\n",
      "  4 stars: 84,660 (24.7%)\n",
      "  5 stars: 220,225 (64.3%)\n",
      "\n",
      "✓ Preprocessing complete!\n",
      "\n",
      "To load the preprocessed data:\n",
      "  import pickle\n",
      "  with open('preprocessed_data.pkl', 'rb') as f:\n",
      "      data = pickle.load(f)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total users: {n_users:,}\")\n",
    "print(f\"Total businesses: {n_businesses:,}\")\n",
    "print(f\"Total interactions: {len(df):,}\")\n",
    "print(f\"\")\n",
    "print(f\"Train interactions: {len(train_df):,}\")\n",
    "print(f\"Val interactions: {len(val_df):,}\")\n",
    "print(f\"Test interactions: {len(test_df):,}\")\n",
    "print(f\"\")\n",
    "print(f\"Avg reviews per user: {len(train_df)/n_users:.2f}\")\n",
    "print(f\"Avg reviews per business: {len(train_df)/n_businesses:.2f}\")\n",
    "print(f\"\")\n",
    "print(f\"Rating distribution (train):\")\n",
    "for rating in sorted(train_df['rating'].unique()):\n",
    "    count = (train_df['rating'] == rating).sum()\n",
    "    pct = count / len(train_df) * 100\n",
    "    print(f\"  {rating} stars: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n✓ Preprocessing complete!\")\n",
    "print(\"\\nTo load the preprocessed data:\")\n",
    "print(\"  import pickle\")\n",
    "print(\"  with open('preprocessed_data.pkl', 'rb') as f:\")\n",
    "print(\"      data = pickle.load(f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 98,975, Items: 28,274\n",
      "Train likes: 304,885\n",
      "Test likes: 130,465\n"
     ]
    }
   ],
   "source": [
    "with open(\"preprocessed_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "train_df = data[\"train_df\"][[\"user_idx\", \"business_idx\", \"rating\"]]\n",
    "test_df = data[\"test_df\"][[\"user_idx\", \"business_idx\", \"rating\"]]\n",
    "n_users = data[\"n_users\"]\n",
    "n_items = data[\"n_businesses\"]\n",
    "\n",
    "# Use likes (rating >= 4) for implicit feedback\n",
    "train_likes = train_df[train_df[\"rating\"] >= 4]\n",
    "test_likes = test_df[test_df[\"rating\"] >= 4]\n",
    "\n",
    "print(f\"Users: {n_users:,}, Items: {n_items:,}\")\n",
    "print(f\"Train likes: {len(train_likes):,}\")\n",
    "print(f\"Test likes: {len(test_likes):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build User-Item Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building graph structure...\n",
      "✓ Built graph with 609770 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1426499/3865854288.py:32: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv_sqrt = np.power(rowsum, -0.5)\n"
     ]
    }
   ],
   "source": [
    "def build_sparse_graph(train_df, n_users, n_items):\n",
    "    \"\"\"\n",
    "    Build normalized adjacency matrix for LightGCN\n",
    "    Graph structure:\n",
    "    [  0      R  ]\n",
    "    [ R^T     0  ]\n",
    "    \n",
    "    Where R is user-item interaction matrix\n",
    "    \"\"\"\n",
    "    users = train_df['user_idx'].values\n",
    "    items = train_df['business_idx'].values\n",
    "    \n",
    "    # Build bipartite graph directly as COO (faster than DOK)\n",
    "    print(\"  Building graph structure...\")\n",
    "    # User -> Item edges\n",
    "    row_ui = users\n",
    "    col_ui = items + n_users  # Offset item indices\n",
    "    # Item -> User edges  \n",
    "    row_iu = items + n_users\n",
    "    col_iu = users\n",
    "    # Combine both directions\n",
    "    row = np.concatenate([row_ui, row_iu])\n",
    "    col = np.concatenate([col_ui, col_iu])\n",
    "    data = np.ones(len(row), dtype=np.float32)\n",
    "    \n",
    "    adj_mat = sp.coo_matrix((data, (row, col)), \n",
    "                            shape=(n_users + n_items, n_users + n_items),\n",
    "                            dtype=np.float32)\n",
    "    \n",
    "    # Normalize: D^(-1/2) * A * D^(-1/2)\n",
    "    rowsum = np.array(adj_mat.sum(1)).flatten()\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5)\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    \n",
    "    # Multiply by diagonal matrix efficiently\n",
    "    row_normalized = d_inv_sqrt[row] * data * d_inv_sqrt[col]\n",
    "    \n",
    "    norm_adj = sp.coo_matrix((row_normalized, (row, col)), \n",
    "                             shape=(n_users + n_items, n_users + n_items),\n",
    "                             dtype=np.float32)\n",
    "    \n",
    "    # Convert to torch sparse tensor\n",
    "    indices = torch.LongTensor(np.vstack([norm_adj.row, norm_adj.col]))\n",
    "    values = torch.FloatTensor(norm_adj.data)\n",
    "    shape = torch.Size(norm_adj.shape)\n",
    "    \n",
    "    return torch.sparse_coo_tensor(indices, values, shape)\n",
    "\n",
    "graph = build_sparse_graph(train_likes, n_users, n_items)\n",
    "print(f\"✓ Built graph with {graph._nnz()} edges\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LightGCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    LightGCN: Simplified Graph Convolutional Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_users, n_items, embedding_dim=64, n_layers=3, reg_weight=1e-4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.reg_weight = reg_weight\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        # Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "        \n",
    "        print(f\"  Embedding dim: {embedding_dim}\")\n",
    "        print(f\"  Layers: {n_layers}\")\n",
    "        print(f\"  Total parameters: {(n_users + n_items) * embedding_dim:,}\")\n",
    "    \n",
    "    def get_ego_embeddings(self):\n",
    "        \"\"\"Get initial embeddings (layer 0)\"\"\"\n",
    "        user_emb = self.user_embedding.weight\n",
    "        item_emb = self.item_embedding.weight\n",
    "        ego_emb = torch.cat([user_emb, item_emb], dim=0)\n",
    "        return ego_emb\n",
    "    \n",
    "    def forward(self, graph):\n",
    "        \"\"\"\n",
    "        Graph convolution to get final embeddings\n",
    "        \"\"\"\n",
    "        all_embeddings = [self.get_ego_embeddings()]\n",
    "        \n",
    "        # Multi-layer propagation\n",
    "        for layer in range(self.n_layers):\n",
    "            # Graph convolution: aggregate from neighbors\n",
    "            ego_emb = all_embeddings[-1]\n",
    "            if graph.device.type == 'cpu':\n",
    "                side_emb = torch.sparse.mm(graph, ego_emb.cpu()).to(ego_emb.device)\n",
    "            else:\n",
    "                side_emb = torch.sparse.mm(graph, ego_emb)\n",
    "            all_embeddings.append(side_emb)\n",
    "        \n",
    "        # Layer aggregation (mean of all layers)\n",
    "        final_emb = torch.stack(all_embeddings, dim=1).mean(dim=1)\n",
    "        \n",
    "        # Split back to users and items\n",
    "        users_emb = final_emb[:self.n_users]\n",
    "        items_emb = final_emb[self.n_users:]\n",
    "        \n",
    "        return users_emb, items_emb\n",
    "    \n",
    "    def bpr_loss(self, users, pos_items, neg_items, user_emb, item_emb):\n",
    "        \"\"\"\n",
    "        BPR loss: maximize difference between positive and negative items\n",
    "        \"\"\"\n",
    "        # Get embeddings\n",
    "        u_emb = user_emb[users]\n",
    "        pos_emb = item_emb[pos_items]\n",
    "        neg_emb = item_emb[neg_items]\n",
    "        \n",
    "        # Compute scores\n",
    "        pos_scores = (u_emb * pos_emb).sum(dim=1)\n",
    "        neg_scores = (u_emb * neg_emb).sum(dim=1)\n",
    "        \n",
    "        # BPR loss\n",
    "        bpr_loss = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10).mean()\n",
    "        \n",
    "        # L2 regularization\n",
    "        reg_loss = self.reg_weight * (\n",
    "            u_emb.norm(2).pow(2) + \n",
    "            pos_emb.norm(2).pow(2) + \n",
    "            neg_emb.norm(2).pow(2)\n",
    "        ) / len(users)\n",
    "        \n",
    "        return bpr_loss + reg_loss\n",
    "    \n",
    "    def predict(self, users, items, user_emb, item_emb):\n",
    "        \"\"\"Predict scores for user-item pairs\"\"\"\n",
    "        u_emb = user_emb[users]\n",
    "        i_emb = item_emb[items]\n",
    "        scores = (u_emb * i_emb).sum(dim=1)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training pairs: 304,885\n"
     ]
    }
   ],
   "source": [
    "class BPRDataset(Dataset):\n",
    "    def __init__(self, train_df, n_items, num_negatives=1):\n",
    "        self.n_items = n_items\n",
    "        self.num_negatives = num_negatives\n",
    "        \n",
    "        # Build user positive items dict\n",
    "        from collections import defaultdict\n",
    "        self.user_pos_items = defaultdict(set)\n",
    "        for _, row in train_df.iterrows():\n",
    "            self.user_pos_items[int(row.user_idx)].add(int(row.business_idx))\n",
    "        \n",
    "        # Create training pairs\n",
    "        self.users = []\n",
    "        self.pos_items = []\n",
    "        \n",
    "        for user, items in self.user_pos_items.items():\n",
    "            for item in items:\n",
    "                self.users.append(user)\n",
    "                self.pos_items.append(item)\n",
    "        \n",
    "        self.all_items = set(range(n_items))\n",
    "        print(f\"  Training pairs: {len(self.users):,}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        user = self.users[idx]\n",
    "        pos_item = self.pos_items[idx]\n",
    "        \n",
    "        # Sample negative items\n",
    "        neg_items = []\n",
    "        pos_set = self.user_pos_items[user]\n",
    "        candidates = self.all_items - pos_set\n",
    "        \n",
    "        if len(candidates) > 0:\n",
    "            neg_items = np.random.choice(list(candidates), \n",
    "                                        size=min(self.num_negatives, len(candidates)), \n",
    "                                        replace=False)\n",
    "        else:\n",
    "            neg_items = [np.random.randint(0, self.n_items)]\n",
    "        \n",
    "        return user, pos_item, neg_items[0]\n",
    "\n",
    "train_dataset = BPRDataset(train_likes, n_items, num_negatives=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Device: cuda:0\n",
      "  Embedding dim: 64\n",
      "  Layers: 3\n",
      "  Total parameters: 8,143,936\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 38/38 [02:39<00:00,  4.20s/it, loss=0.6914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss = 0.6926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 38/38 [02:40<00:00,  4.21s/it, loss=0.6743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Average Loss = 0.6849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 38/38 [02:39<00:00,  4.21s/it, loss=0.6243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Average Loss = 0.6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 38/38 [02:40<00:00,  4.23s/it, loss=0.5468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Average Loss = 0.5844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 38/38 [02:38<00:00,  4.17s/it, loss=0.4644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Average Loss = 0.5021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 38/38 [02:38<00:00,  4.16s/it, loss=0.4015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Average Loss = 0.4251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 38/38 [02:40<00:00,  4.22s/it, loss=0.3311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Average Loss = 0.3621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 38/38 [02:37<00:00,  4.15s/it, loss=0.2909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Average Loss = 0.3128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 38/38 [02:38<00:00,  4.16s/it, loss=0.2583]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Average Loss = 0.2754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 38/38 [02:40<00:00,  4.23s/it, loss=0.2300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Average Loss = 0.2458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 38/38 [02:36<00:00,  4.12s/it, loss=0.2097]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Average Loss = 0.2216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 38/38 [02:37<00:00,  4.13s/it, loss=0.1970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Average Loss = 0.2026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 38/38 [02:35<00:00,  4.10s/it, loss=0.1782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Average Loss = 0.1866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 38/38 [02:38<00:00,  4.18s/it, loss=0.1644]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Average Loss = 0.1727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 38/38 [02:37<00:00,  4.15s/it, loss=0.1560]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Average Loss = 0.1608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 38/38 [02:39<00:00,  4.20s/it, loss=0.1498]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Average Loss = 0.1512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 38/38 [02:36<00:00,  4.12s/it, loss=0.1348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Average Loss = 0.1413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 38/38 [02:37<00:00,  4.15s/it, loss=0.1314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Average Loss = 0.1331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 38/38 [02:37<00:00,  4.15s/it, loss=0.1311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Average Loss = 0.1265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 38/38 [02:36<00:00,  4.11s/it, loss=0.1135]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Average Loss = 0.1198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "N_LAYERS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "REG_WEIGHT = 1e-4\n",
    "BATCH_SIZE = 2048\n",
    "EPOCHS = 20\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = LightGCN(n_users, n_items, EMBEDDING_DIM, N_LAYERS, REG_WEIGHT)\n",
    "\n",
    "model = model.to(device)\n",
    "graph = graph.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    "# Training\n",
    "print(\"\\nStarting training...\")\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    for batch_idx, (users, pos_items, neg_items) in enumerate(pbar):\n",
    "        users = users.to(device)\n",
    "        pos_items = pos_items.to(device)\n",
    "        neg_items = neg_items.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get fresh embeddings for each batch (recompute graph convolution)\n",
    "        user_emb, item_emb = model.forward(graph)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = model.bpr_loss(users, pos_items, neg_items, user_emb, item_emb)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}: Average Loss = {avg_loss:.4f}\")\n",
    "    \n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save(model.state_dict(), 'best_lightgcn.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1426499/1842534484.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_lightgcn.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 2000 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGCN Recall@10: 0.0452\n",
      "LightGCN NDCG@10: 0.0278\n"
     ]
    }
   ],
   "source": [
    "def recall_at_k(recommended, ground_truth, K):\n",
    "    if len(ground_truth) == 0:\n",
    "        return None\n",
    "    hit = len(set(recommended[:K]) & set(ground_truth))\n",
    "    return hit / len(ground_truth)\n",
    "\n",
    "def ndcg_at_k(recommended, ground_truth, K):\n",
    "    dcg = 0.0\n",
    "    for rank, item in enumerate(recommended[:K], start=1):\n",
    "        if item in ground_truth:\n",
    "            dcg += 1 / math.log2(rank + 1)\n",
    "    max_rel = min(K, len(ground_truth))\n",
    "    idcg = sum(1 / math.log2(rank + 1) for rank in range(1, max_rel + 1))\n",
    "    return dcg / idcg if idcg > 0 else None\n",
    "\n",
    "def evaluate_lightgcn(model, graph, test_df, train_likes, K=10, \n",
    "                      sample_users=2000, device='cuda:0', batch_size=8192):\n",
    "    \"\"\"Evaluate LightGCN on test set\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get final embeddings\n",
    "    user_emb, item_emb = model.forward(graph)\n",
    "    \n",
    "    # Build user seen items\n",
    "    user_seen = train_likes.groupby(\"user_idx\")[\"business_idx\"].apply(set).to_dict()\n",
    "    \n",
    "    # Build test ground truth (rating >= 4)\n",
    "    test_filtered = test_df[test_df['rating'] >= 4]\n",
    "    test_items_by_user = test_filtered.groupby('user_idx')['business_idx'].apply(set).to_dict()\n",
    "    test_users = list(test_items_by_user.keys())\n",
    "    \n",
    "    # Sample users\n",
    "    if sample_users and sample_users < len(test_users):\n",
    "        np.random.seed(42)\n",
    "        test_users = np.random.choice(test_users, size=sample_users, replace=False)\n",
    "    \n",
    "    recalls, ndcgs = [], []\n",
    "    all_items = torch.arange(n_items, device=device)\n",
    "    \n",
    "    print(f\"Evaluating {len(test_users)} users...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(test_users), batch_size)):\n",
    "        batch_users = test_users[i:i+batch_size]\n",
    "        batch_user_tensor = torch.LongTensor(batch_users).to(device)\n",
    "        \n",
    "        # Get user embeddings\n",
    "        batch_user_emb = user_emb[batch_user_tensor]\n",
    "        \n",
    "        # Compute scores for all items\n",
    "        scores = torch.matmul(batch_user_emb, item_emb.T)\n",
    "        \n",
    "        # Mask seen items\n",
    "        for j, user in enumerate(batch_users):\n",
    "            if user in user_seen:\n",
    "                seen_items = list(user_seen[user])\n",
    "                scores[j, seen_items] = -float('inf')\n",
    "        \n",
    "        # Get top-K\n",
    "        _, top_k_items = torch.topk(scores, k=K, dim=1)\n",
    "        top_k_items = top_k_items.cpu().numpy()\n",
    "        \n",
    "        # Compute metrics\n",
    "        for j, user in enumerate(batch_users):\n",
    "            recs = top_k_items[j].tolist()\n",
    "            truth = list(test_items_by_user.get(user, set()))\n",
    "            \n",
    "            r = recall_at_k(recs, truth, K)\n",
    "            n = ndcg_at_k(recs, truth, K)\n",
    "            \n",
    "            if r is not None:\n",
    "                recalls.append(r)\n",
    "            if n is not None:\n",
    "                ndcgs.append(n)\n",
    "    \n",
    "    return np.mean(recalls), np.mean(ndcgs)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_lightgcn.pt'))\n",
    "\n",
    "# Evaluate\n",
    "recall, ndcg = evaluate_lightgcn(model, graph, test_df, train_likes, \n",
    "                                 K=10, sample_users=2000, device=device)\n",
    "\n",
    "print(f\"\\nLightGCN Recall@10: {recall:.4f}\")\n",
    "print(f\"LightGCN NDCG@10: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
